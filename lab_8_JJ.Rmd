---
title: "Lab 8: Sentiment Analysis and Text Mining"
author: "Jessica Jagdeo"
date: "2/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

#### Attach packages:
```{r}

library(tidyverse)
library(here)
library(janitor)

# For text mining:

library(pdftools)
library(tidytext)
library(textdata)
library(ggwordcloud)

```

#### Read in the report:
```{r}

ipcc_path <- here("data", "ipcc_gw_15.pdf")

ipcc_text <- pdf_text(ipcc_path)

ipcc_p9 <- ipcc_text[9]

ipcc_p9 # \r\n indicates a line break within the PDF

```

#### Get this into dataframe shape and do some wrangling:

- Split up pages into separate lines (using '\r\n') using 'stringr::str_split()'
- Unnest into regular columns using 'tidyr::unnest()' 
- Remove leading/trailing white space using 'stringr::str_trim()'

```{r}

ipcc_df <- data.frame(ipcc_text) %>% 
  mutate(text_full = str_split(ipcc_text, pattern = "\r\n")) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full))

```

#### Get tokens using 'unnest_tokens()'
```{r}

ipcc_tokens <- ipcc_df %>% 
  unnest_tokens(word, text_full)

```

#### Count all the words:
```{r}

ipcc_wc <- ipcc_tokens %>% 
  count(word) %>% 
  arrange(-n)

```

#### Remove the stop words:
```{r}

ipcc_stop <- ipcc_tokens %>% 
  anti_join(stop_words) %>% 
  dplyr::select(-ipcc_text)

```

#### Remove all the numeric pieces:
```{r}

ipcc_no_numeric <- ipcc_stop %>% 
  dplyr::filter(is.na(as.numeric(word))) # Convert all entries to numeric. If the entry is not a numeric, it returns 'NA'. If an entry is 'NA', retain it through the filter. 

```

#### Start doing some visualization:

Word cloud
```{r}

ipcc_top100 <- ipcc_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)

ipcc_cloud <- ggplot(ipcc_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_classic()

ipcc_cloud

ggplot(ipcc_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen", "blue", "purple")) +
  theme_classic()

```

#### Sentiment analysis for text:

Let's explore the sentiment lexicons. "bing" included, other lexicons ("afinn", "nrc", "loughran") you'll be prompted to to download.

```{r}
get_sentiments(lexicon = "afinn")

get_sentiments(lexicon = "bing")

get_sentiments(lexicon = "nrc")

```

"afinn": Words ranked from -5 (very negative) to +5 (very positive)

### Bind together words:
```{r}

ipcc_afinn <- ipcc_stop %>% 
  inner_join(get_sentiments(lexicon = "afinn"))

```

### Find counts of value rankings: 
```{r}

ipcc_afinn_hist <- ipcc_afinn %>% 
  count(value) %>% 
  ggplot() +
  geom_col(aes(x = value, y = n))

ipcc_afinn_hist

```

```{r}

ipcc_afinn2 <- ipcc_afinn %>% 
  filter(value == 2)

```

```{r}

ipcc_summary <-  ipcc_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )

```

#### Check out sentiments by NRC
```{r}

ipcc_nrc <- ipcc_stop %>% 
  inner_join(get_sentiments(lexicon = "nrc"))

# See what's excluded:

ipcc_exclude <- ipcc_stop %>% 
  anti_join(get_sentiments(lexicon = "nrc"))

```

#### Find counts by sentiment:
```{r}

ipcc_nrc_n <- ipcc_nrc %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(sentiment = as.factor(sentiment)) %>% 
  mutate(sentiment = fct_reorder(sentiment, -n))

ggplot(ipcc_nrc_n) +
  geom_col(aes(x = sentiment, y = n)) +
  theme_classic()

```

For each sentiment bin, what are the top 5 most frequent words associated with that bin?

```{r}

ipcc_nrc_n5 <- ipcc_nrc %>% 
  count(word, sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5)

ipcc_nrc_gg <- ggplot(data = ipcc_nrc_n5,
                      aes(x = reorder(word, n),
                          y = n,
                          fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free")

ipcc_nrc_gg

```

